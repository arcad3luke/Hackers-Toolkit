# ! /usr/bin/env python
# -*- coding: utf-8 -*-
#
# Support module generated by PAGE version 7.5
# in conjunction with Tcl version 8.6
# Nov 28, 2022 09:05:29 AM EST  platform: Windows NT

import sys
import tkinter as tk
import SwipeDown


def main():
    """Main entry point for the application."""
    global _root
    _root = tk.Tk()
    _root.protocol('WM_DELETE_WINDOW', _root.destroy)
    # Creates a toplevel widget.
    global _top1, _w1
    _top1 = _root
    _w1 = SwipeDown.SwipeDown(_top1)
    _root.mainloop()


def sqlInject(*args):
    print('SwipeDown_support.def sqlInject')
    for arg in args:
        print('	another arg:', arg)
    sys.stdout.flush()


def xxx(*args):
    print('SwipeDown_support.xxx')
    for arg in args:
        print('	another arg:', arg)

    sys.stdout.flush()


def scrape():
    # Import necessary libraries
    import json

    import pandas as pd
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from bs4 import BeautifulSoup

    # Set up the Chrome driver
    s = Service('C:\Program Files\Google\Chrome\Application\chrome.exe')
    driver = webdriver.Chrome(service=s)

    # Navigate to the webpage to scrape
    dork = json.loads('GoogleHackMasterList.csv')
    for x in dork:
        driver.get(f'https://google.com/&q={x}')

    # Get the page source and parse it with Beautiful Soup
    page_source = driver.page_source
    soup = BeautifulSoup(page_source, 'html.parser')

    # Find the elements on the page to scrape
    titles = soup.find_all('h3', class_='title')
    links = soup.find_all('a', class_='link')
    descriptions = soup.find_all('p', class_='description')

    # Create an empty DataFrame to store the scraped data
    df = pd.DataFrame(columns=['Title', 'Link', 'Description'])

    # Loop through the scraped elements and extract the necessary information
    for i in range(len(titles)):
        title = titles[i].text
        link = links[i]['href']
        description = descriptions[i].text

        # Append the information to the DataFrame
        df = df.append({'Title': title, 'Link': link, 'Description': description}, ignore_index=True)

    # Close the Chrome driver
    driver.quit()

    # Save the scraped data to a CSV file
    df.to_csv('scraped_data.csv', index=False)


def httpPollute():
    """
     @TODO
     Learn HTTP Parameter Pollution and write a script.
     """


def cssInject():
    """
    @TODO
    learn CSS Injection and create a script for it.
    """


if __name__ == '__main__':
    SwipeDown.start_up()
